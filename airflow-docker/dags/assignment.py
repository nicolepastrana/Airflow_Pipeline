from airflow import DAG
from datetime import timedelta
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
from airflow.operators.python import PythonOperator
import urllib.request
import time
import glob, os
import json

# t1 - pull course catalog pages
def catalog():

    #define pull(url) helper function
    def pull(url):
        response = urllib.request.urlopen(url).read()
        data = response.decode('utf-8')
        return data
         
    #define store(data,file) helper function
    def store(data,file):
        f = open(file,'w+')
        f.write(data)
        f.close()
        print('wrote file: ' + file)
    
    # # Read .txt file into list for iteration.
    # with open('00_urls.txt') as u:
    #     urls_file = u.read().splitlines()

    # # Pull data only if URL is valid.
    import requests
    
    urls = ['http://student.mit.edu/catalog/m1a.html', 'http://student.mit.edu/catalog/m1b.html', 'http://student.mit.edu/catalog/m1c.html', 'http://student.mit.edu/catalog/m2a.html', 'http://student.mit.edu/catalog/m2b.html', 'http://student.mit.edu/catalog/m2c.html', 'http://student.mit.edu/catalog/m3a.html', 'http://student.mit.edu/catalog/m3b.html', 'http://student.mit.edu/catalog/m4a.html', 'http://student.mit.edu/catalog/m4b.html', 'http://student.mit.edu/catalog/m4c.html', 'http://student.mit.edu/catalog/m4d.html', 'http://student.mit.edu/catalog/m4e.html', 'http://student.mit.edu/catalog/m4f.html', 'http://student.mit.edu/catalog/m4g.html', 'http://student.mit.edu/catalog/m5a.html', 'http://student.mit.edu/catalog/m5b.html', 'http://student.mit.edu/catalog/m6a.html', 'http://student.mit.edu/catalog/m6b.html', 'http://student.mit.edu/catalog/m6c.html', 'http://student.mit.edu/catalog/m7a.html', 'http://student.mit.edu/catalog/m8a.html', 'http://student.mit.edu/catalog/m8b.html', 'http://student.mit.edu/catalog/m9a.html', 'http://student.mit.edu/catalog/m9b.html', 'http://student.mit.edu/catalog/m10a.html', 'http://student.mit.edu/catalog/m10b.html', 'http://student.mit.edu/catalog/m10c.html', 'http://student.mit.edu/catalog/m11a.html', 'http://student.mit.edu/catalog/m11b.html', 'http://student.mit.edu/catalog/m11c.html', 'http://student.mit.edu/catalog/m12a.html', 'http://student.mit.edu/catalog/m12b.html', 'http://student.mit.edu/catalog/m12c.html', 'http://student.mit.edu/catalog/m14a.html', 'http://student.mit.edu/catalog/m14b.html', 'http://student.mit.edu/catalog/m15a.html', 'http://student.mit.edu/catalog/m15b.html', 'http://student.mit.edu/catalog/m15c.html', 'http://student.mit.edu/catalog/m16a.html', 'http://student.mit.edu/catalog/m16b.html', 'http://student.mit.edu/catalog/m18a.html', 'http://student.mit.edu/catalog/m18b.html', 'http://student.mit.edu/catalog/m20a.html', 'http://student.mit.edu/catalog/m22a.html', 'http://student.mit.edu/catalog/m22b.html', 'http://student.mit.edu/catalog/m22c.html']
    
    for url in urls:
        get = requests.get(url)
        if get.status_code == 200:
            index = url.rfind('/') + 1
            file = url[index:]
            store(pull(url),file)
            print('--- reading... ---')
            time.sleep(15)


# t2 - combine all data files
def combine():
    with open('combo.txt','w') as outfile:
        for file in glob.glob("*.html"):
            with open(file) as infile:
                outfile.write(infile.read())


# t3 - scrape h3 titles
def titles():
    from bs4 import BeautifulSoup

    def store_json(data,file):
        with open(file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            print('added to: ' + file)

    #Open and read the large html file generated by combine()
    file = open('combo.txt','r')
    html = file.read()
    html = html.replace('\n', ' ').replace('\r', '')
    soup = BeautifulSoup(html, "html.parser")
    results = soup.find_all('h3')
    titles = []
    for item in results:
        titles.append(item.text)
    store_json(titles, 'h3_titles_raw.json')


# t4 - remove unwanted characters from titles
def clean():

    def store_json(data,file):
        with open(file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            print('cleaned titles: ' + file)
    
    with open('h3_titles_raw.json','r') as file:
        titles = json.load(file)
       # remove punctuation/numbers
        for index, title in enumerate(titles):
            punctuation = '''!()-[]{};:'"\,<>./?@#$%^&*_~1234567890'''
            translationTable = str.maketrans("","",punctuation)
            clean = title.translate(translationTable)
            titles[index] = clean
       # remove one character words
        for index, title in enumerate(titles):
            clean = ' '.join( [word for word in title.split() if len(word)>1] )
            titles[index] = clean

        store_json(titles, 'h3_titles_clean.json')


# t5 - word count
def count_words():
    from collections import Counter

    def store_json(data,file):
        with open(file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            print('counted words: ' + file)

    with open('h3_titles_clean.json','r') as file:
            titles = json.load(file)
            words = []
            # extract words and flatten
            for title in titles:
                words.extend(title.split())
            # count word frequency
            counts = Counter(words)
            store_json(counts, 'words.json')


# Airflow pipeline
with DAG(
   "assignment",
   start_date=days_ago(1),
   schedule_interval="@daily",catchup=False,
   ) as dag:

    t0 = BashOperator(
       task_id='task_zero',
       bash_command='pip install beautifulsoup4',
       retries=2
       )
    t1 = PythonOperator(
       task_id='task_one',
       depends_on_past=False,
       python_callable=catalog
       )
    t2 = PythonOperator(
       task_id='task_two',
       depends_on_past=False,
       python_callable=combine
       )
    t3 = PythonOperator(
       task_id='task_three',
       depends_on_past=False,
       python_callable=titles
       )
    t4 = PythonOperator(
       task_id='task_four',
       depends_on_past=False,
       python_callable=clean
       )
    t5 = PythonOperator(
       task_id='task_five',
       depends_on_past=False,
       python_callable=count_words
       )
    
    t0>>t1>>t2>>t3>>t4>>t5